{"componentChunkName":"component---src-templates-docs-js","path":"/google-cloud-platform/pubsub-streaming-to-bigquery","result":{"data":{"site":{"siteMetadata":{"title":"Gatsby Gitbook Boilerplate | Hasura","docsLocation":"https://github.com/bodhi-root/public-wiki/tree/master/content"}},"mdx":{"fields":{"id":"c2b6b9aa-66f8-587c-8e67-0d12c3444f15","title":"PubSub Streaming to BigQuery","slug":"/google-cloud-platform/pubsub-streaming-to-bigquery"},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"PubSub Streaming to BigQuery\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"One design pattern we are trying to build is an event stream published to PubSub that is then streamed into BigQuery.  Several methods exist for reading from PubSub and writing to BigQuery:\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Method\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Description\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Pros\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Cons\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Cloud Functions\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"A NodeJS function that would be invoked each time a new message arrives in the PubSub topic. This would insert a row into BigQuery. (Recommended by our Google rep. \", mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://github.com/kevensen/professional-services/tree/simple-pipeline/examples/simple-pipeline/cloud_function\"\n  }), \"Link to code\"), \")\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Relatively simple.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Bruce pointed out that this is a table load operation and would limit us to 10,000 inserts per table/day.\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Dataflow\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Streaming Dataflow pipeline (written in Java) (Recommended by the book \\\"Data Science on the Google Cloud Platform\\\".)\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  })), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"GKE\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Docker pod deployed through Kubernetes to read from PubSub and write to BigQuery. Program can be simple Python.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  })), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"(Recommended by the Google tutorial at: \", mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://github.com/GoogleCloudPlatform/kubernetes-bigquery-python/tree/master/pubsub\"\n  }), \"Link to code\"), \")\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Kubernetes would let us scale the streaming code to as many nodes as needed. Also manages deployment and can auto-restart nodes as needed.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Kubernetes is kind of a pain.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"GCE\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Deploy a program on Compute Engine that reads from PubSub and writes to BigQuery. Program can be simple Python.\\tSimple.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"Harder to monitor/support than Kubernetes.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }))))), mdx(\"h2\", null, \"Google Tutorial using GKE\"), mdx(\"p\", null, \"Google has a tutorial for accomplishing this using GKE:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://github.com/GoogleCloudPlatform/kubernetes-bigquery-python/tree/master/pubsub\"\n  }), \"https://github.com/GoogleCloudPlatform/kubernetes-bigquery-python/tree/master/pubsub\"))), mdx(\"p\", null, \"While most of the steps in this tutorial were relatively straight-forward, I did run into problems when I tried to modify the code to listen for new keywords.  First, I had never compiled docker images before.  This quick start guide was very helpful in walking me through the process of building the image and pushing it to Google's container repository.  The commands to do this were:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"docker build -t pubsub-bq-pipe .\\ndocker tag pubsub-bq-pipe gcr.io/futurestats-grocery/pubsub-bq-pipe:v1\\ndocker push gcr.io/futurestats-grocery/pubsub-bq-pipe:v1\\n\")), mdx(\"p\", null, \"When I deployed this to Kubernetes I ran into a problem with the pods crashing though.  I learned how to view a pods logs with:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"kubectl logs bigquery-controller-8679747dfc-dgpjk\\n\")), mdx(\"p\", null, \"and then saw this error:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Traceback (most recent call last):\\n  File \\\"pubsub-to-bigquery.py\\\", line 25, in <module>\\n    import utils\\n  File \\\"/utils.py\\\", line 27, in <module>\\n    from oauth2client.client import GoogleCredentials\\nImportError: No module named oauth2client.client\\n\")), mdx(\"p\", null, \"For some reason the Dockerfile was not including this necessary dependency.  I found some help online and modified the Dockerfile to include a line to install and upgrade oauth2client.  My Dockerfile then looked like this:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"Dockerfile\\nFROM python:2\\n\\nRUN pip install --upgrade pip\\nRUN pip install tweepy\\nRUN pip install --upgrade oauth2client\\nRUN pip install --upgrade google-api-python-client\\nRUN pip install python-dateutil\\n\\nADD twitter-to-pubsub.py /twitter-to-pubsub.py\\nADD pubsub-to-bigquery.py /pubsub-to-bigquery.py\\nADD controller.py /controller.py\\nADD utils.py /utils.py\\n\\nCMD python controller.py\\n\")), mdx(\"p\", null, \"Re-building the image, pushing to GCR, and re-deploying to Kubernetes was not fixing the problem though.  I kept getting the same error message each time.  Finally, I realized I had to upgrade the image tag to \\\"v2\\\".  Then I had to modify my Kubernetes deployment descriptors to point to this, and then everything worked.\"), mdx(\"p\", null, \"WARNING: It seems that Kubernetes is using some kind of image cache that wasn't recognizing changes when I kept the same version tag. I could see in GCR that I had pushed a new image with the old tag. I could even see the old image, and I deleted it so that it wouldn't be pulling it by mistake. But it was somehow still deploying the old image. Not until I changed the tag to \\\"v2\\\" did everything work out.\"), mdx(\"h3\", null, \"Final Code Files\"), mdx(\"p\", null, \"For reference, here are the files I finally got working.  The twitter-to-pubsub.py script was modified to change the filter:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"FILE: twitter-to-pubsub.py (snippet)\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"if os.environ['TWSTREAMMODE'] == 'sample':\\n    stream.sample()\\nelse:\\n    stream.filter(\\n       track=['kroger','publix','costco','heb','walmart','meijer','aldi','lidl','whole foods','hyvee','wegmans','grocery','groceries']\\n    )\\n\")), mdx(\"p\", null, \"Note that if the environmental variable \\\"TWSTREAMMODE\\\" is \\\"sample\\\" that this code will never be invoked.  That was something else I missed the first time.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"FILE: bigquery-controller.yaml\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"bigquery-controller.yaml\\napiVersion: apps/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: bigquery-controller\\n  labels:\\n    name: bigquery-controller\\nspec:\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        name: bigquery-controller\\n    spec:\\n      containers:\\n      - name: bigquery\\n        image: gcr.io/futurestats-grocery/pubsub-bq-pipe:v2\\n        env:\\n        - name: PROCESSINGSCRIPT\\n          value: pubsub-to-bigquery\\n        - name: PUBSUB_TOPIC\\n          value: projects/futurestats-grocery/topics/twitter_stream\\n        - name: PROJECT_ID\\n          value: futurestats-grocery\\n        - name: BQ_DATASET\\n          value: twitter_stream\\n        - name: BQ_TABLE\\n          value: tweets\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"FILE: twitter-stream.yaml\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"apiVersion: apps/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: twitter-stream\\n  labels:\\n    name: twitter-stream\\nspec:\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        name: twitter-stream\\n    spec:\\n      containers:\\n      - name: twitter-to-pubsub\\n        image: gcr.io/futurestats-grocery/pubsub-bq-pipe:v2\\n        env:\\n        - name: PROCESSINGSCRIPT\\n          value: twitter-to-pubsub\\n        - name: PUBSUB_TOPIC\\n          value: projects/futurestats-grocery/topics/twitter_stream\\n        - name: CONSUMERKEY\\n          value: your_info_here\\n        - name: CONSUMERSECRET\\n          value: your_info_here\\n        - name: ACCESSTOKEN\\n          value: your_info_here\\n        - name: ACCESSTOKENSEC\\n          value: your_info_here\\n        - name: TWSTREAMMODE\\n          value: prod\\n\")), mdx(\"p\", null, \"Note that the last variable, \\\"TWSTREAMMODE\\\" was set to \\\"prod\\\".  Really, anything other than \\\"sample\\\" with work here.  You will also need to insert your own Twitter app credentials, of course.\"), mdx(\"p\", null, \"Unfortunately, I wasn't able to put this code into source control.  I don't want my Twitter credentials to be stored there.  \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/\"\n  }), \"This page\"), \" has a good example of how to secretly and securely set environment variables, but that will have to be an exercise for another day.\"), mdx(\"h3\", null, \"GCP Costs\"), mdx(\"p\", null, \"I left this running until 2/21 (7 days) and ran up a bill of $26.05.  It looked like it was charging about $4 per day.  The primary costs were for compute.  Details below:\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1035px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/public-wiki/static/44392ec3ab8a1d1d0d18ef07662cffe2/c5650/gcp-costs-pubsub-to-bigquery.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": [\"noopener\"]\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"37.45885450954575%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABAklEQVQoz2VRi27EIAzj/7+xN+n20KRtt/IutIAvDutN2pAs4zRxSGrGGAAGTv4Lhm0a+LQ73r8y3j4ibjbCewfnvLAXdogxqodpvaO1jt6H8kR78PfN4ro84XlZBBe5X/ByfYW1To0Iay1CCNPwkMKUiwS9ct0PlPqLnDJq8oo9B0WOwvv+DzymVoqqgeM4FKUU1cqiq9ybTLCVirxt8k0aZWlUKzbRzKMmjF0D1nXVHRApJdVM5ChM5ljaSGJOYjTi7phz1mWpY8yUaOFl/u2nI19GUxokGXfy1DzcK3fVe3+AMR7mmPOp5w6YQANqdiaH4NWEoGYOi+fPa3rnNz7oDlNHISCVmyM/AAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Cost Screenshot\",\n    \"title\": \"Cost Screenshot\",\n    \"src\": \"/public-wiki/static/44392ec3ab8a1d1d0d18ef07662cffe2/bcbcb/gcp-costs-pubsub-to-bigquery.png\",\n    \"srcSet\": [\"/public-wiki/static/44392ec3ab8a1d1d0d18ef07662cffe2/8472d/gcp-costs-pubsub-to-bigquery.png 259w\", \"/public-wiki/static/44392ec3ab8a1d1d0d18ef07662cffe2/9eb08/gcp-costs-pubsub-to-bigquery.png 518w\", \"/public-wiki/static/44392ec3ab8a1d1d0d18ef07662cffe2/bcbcb/gcp-costs-pubsub-to-bigquery.png 1035w\", \"/public-wiki/static/44392ec3ab8a1d1d0d18ef07662cffe2/c5650/gcp-costs-pubsub-to-bigquery.png 1519w\"],\n    \"sizes\": \"(max-width: 1035px) 100vw, 1035px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"In total this collected 892,298 tweets.\"), mdx(\"h2\", null, \"Python Code on Docker\"), mdx(\"p\", null, \"In trying to replicate the Google tutorial for my own project, I ran into several problems.  First, the Python API client for Google cloud is not installed on VMs by default.  I couldn't figure out how to install it either since pip is missing, my VMs seem to be fire-walled off from the internet, and they seem to be firewall-ed off from my company's artifactory (which is needing to install Linux modules and Python modules the correct way).  I thought about building docker images on my desktop, uploading them to the cloud, and then running them there, but that's a pretty complicated code/test cycle.  Instead, I just created a sandbox project, logged directly into the VM, and was able to use the full power of the cloud and the internet directly.  Here are my notes.\"), mdx(\"h3\", null, \"Install Python Libraries\"), mdx(\"p\", null, \"This can be done with:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"sudo apt update\\nsudo apt install python3-pip\\nsudo python3 -m pip install --upgrade google-cloud-pubsub\\nsudo python3 -m pip install --upgrade google-cloud-bigquery\\nsudo python3 -m pip install --upgrade oauth2client\\n\")), mdx(\"p\", null, \"This allows us to perform the following imports:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"from google.cloud import pubsub_v1\\nfrom google.cloud import bigquery\\nfrom oauth2client.client import GoogleCredentials\\n\")), mdx(\"p\", null, \"In order to see what specific versions of these modules installed, I used:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"sudo python3 -m pip freeze\\n\")), mdx(\"p\", null, \"This provided the following key information:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"google-cloud-pubsub==0.39.1\\ngoogle-cloud-bigquery==1.10.0\\noauth2client==4.1.3\\n\")), mdx(\"p\", null, \"About 20 other dependencies were also shown, but I'm assuming if we put these key dependencies into a \\\"requirements.txt\\\" file, they will automatically pull in all the rest.\"), mdx(\"h3\", null, \"BigQuery Setup\"), mdx(\"p\", null, \"BigQuery can be setup if we have a JSON file that describes the desired schema.  In this case I created \\\"my-schema.json\\\" and then used the following to create a dataset named \\\"dataset\\\" and a table named \\\"table\\\":\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"bq mk dataset\\nbq mk -t dataset.table my-schema.json\\n\")), mdx(\"h3\", null, \"Python Scripts\"), mdx(\"p\", null, \"The Python scripts weren't too hard to write once the environment was setup.  To make things easy, I wrote them on my desktop and then uploaded them to the VM.  I ended up with the following files:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"pubsub-test.py         # simple listener that listens to PubSub and prints message as they arrive\\nfile-to-pubsub.py      # generate sample data and push to PubSub\\npubsub-to-bigquery.py  # program that listens to PubSub and pushes messages to BigQuery\\n\")), mdx(\"p\", null, \"These scripts still need some work to be production-ready, but in order to keep from having to read through all the crazy GCP documentation again, here they are:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"File: pubsub-test.py\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"import time\\nfrom google.cloud import pubsub_v1\\n\\nPROJECT_ID = \\\"gcp_project_id\\\"\\nSUBSCRIPTION_NAME = \\\"pubsub_subscription_name\\\"\\n\\nsubscriber = pubsub_v1.SubscriberClient()\\n\\nsubscription_path = subscriber.subscription_path(PROJECT_ID, SUBSCRIPTION_NAME)\\n\\ndef callback(message):\\n    print('Received message: {}'.format(message))\\n    message.ack()\\n\\nsubscriber.subscribe(subscription_path, callback=callback)\\n\\nprint('Listening for messages on {}'.format(subscription_path))\\nwhile True:\\n    time.sleep(60)\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"File: file-to-pubsub.py\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"import random\\nimport datetime\\nimport time\\nfrom google.cloud import pubsub_v1\\n\\nPROJECT_ID = \\\"gcp_project_id\\\"\\nTOPIC_NAME = \\\"topic_name\\\"\\n\\n### Load XML messages into memory\\n\\nsample_file = \\\"data/sample/sensor-xml.log\\\"\\nprint(\\\"Reading file: {}\\\".format(sample_file))\\n\\nxml_lines = []\\nwith open(sample_file, \\\"r\\\") as file_in:\\n    for line in file_in:\\n        xml_lines.append(line)\\n\\nprint(\\\"Loaded {} sample messages into memory\\\".format(len(xml_lines)))\\n\\n### Randomly send message to PubSub\\n\\npublisher = pubsub_v1.PublisherClient()\\nFULL_TOPIC_NAME = 'projects/{project_id}/topics/{topic}'.format(\\n    project_id=PROJECT_ID,\\n    topic=TOPIC_NAME\\n)\\n\\nwhile True:\\n    xml_line = random.choice(xml_lines)\\n    message_bytes = xml_line.encode() # default to utf-8\\n\\n    print(\\\"Writing message at {}\\\".format(datetime.datetime.now()))\\n    publisher.publish(FULL_TOPIC_NAME, message_bytes)  \\n\\n    time.sleep(5) # sleep 5 seconds\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"File: pubsub-to-bigquery.py\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"import time\\nfrom google.cloud import pubsub_v1\\nfrom google.cloud import bigquery\\nfrom seventyfive.SensorEvent import SensorEvent\\n\\nPROJECT_ID = \\\"gcp_project_id\\\"\\nSUBSCRIPTION_NAME = \\\"pubsub_subscription_name\\\"\\n\\nBQ_DATASET_ID = 'dataset'\\nBQ_TABLE_NAME = 'table'\\n\\nsubscriber = pubsub_v1.SubscriberClient()\\n\\nbq_client = bigquery.Client()\\nbq_table = bq_client.get_table(bq_client.dataset(BQ_DATASET_ID).table(BQ_TABLE_NAME))\\n\\nsubscription_path = subscriber.subscription_path(PROJECT_ID, SUBSCRIPTION_NAME)\\n\\ndef callback(message):\\n    print('Received message: {}'.format(message))\\n\\n    xml = message.data.decode()\\n    sensor_event = SensorEvent.parse_xml(xml)\\n\\n    bq_rows = sensor_event.to_bq_rows()\\n    errors = bq_client.insert_rows(bq_table, bq_rows)\\n    print(\\\"Errors: \\\".format(errors))\\n\\n    message.ack()\\n\\nsubscriber.subscribe(subscription_path, callback=callback)\\n\\nprint('Listening for messages on {}'.format(subscription_path))\\nwhile True:\\n    time.sleep(60)\\n\")), mdx(\"p\", null, \"Nothing too tricky here.  The work to parse the SensorEvent XML file is encapsulated in the SensorEvent object.  The code that generates rows to insert into BigQuery is also in that object.  These rows are simply dictionaries of key/value pairs for the column names and values we are inserting.  These scripts need some more error handling and then I'll put them in git.  We also need to bundle them up and build them into a docker image, but that will be work for another day.\"), mdx(\"h3\", null, \"Writing to PubSub from On-Prem\"), mdx(\"p\", null, \"The code above writes to PubSub from a GCE instance.  If we want to test writing messages from on-prem (which is what the ESB team will have to do in order to publish to our GCP PubSub topic), we need to make a few small changes.  First, we generate a service account and save it's token to a JSON file.  This service account should have permission to publish to PubSub.  Then we modify our Python code that instantiates the client to use this service account file:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"publisher = pubsub_v1.PublisherClient.from_service_account_file(\\\"service-account-file.json\\\")\\n\")), mdx(\"p\", null, \"You will also need to use the Proxy to connect to GCP.  This can be set through an environmental variable.  If you do it in Python (which is not recommended) it looks like this:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"os.environ[\\\"https_proxy\\\"] = \\\"http://user:pass@proxy.domain.com:8888/\\\"\\n\")), mdx(\"h3\", null, \"Deploying as a Docker Container\"), mdx(\"p\", null, \"WARNING: I can get this to work in the sandbox but not in our corporate project. For some reason, the default application credentials in the corporate project do not appear to have access to the BigQuery APIs.\"), mdx(\"p\", null, \"I tried deploying this app as a docker container using the following Dockerfile:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"File: Dockerfile\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"FROM python:3.7.2-slim-stretch\\n\\nWORKDIR /usr/src/app\\n\\n# Add files to the image, placing them in WORKDIR\\nCOPY requirements.txt ./\\n\\n# Install python packages\\nRUN set -x \\\\\\n    && pip install --upgrade pip \\\\\\n    && pip install --no-cache -r requirements.txt\\n\\nCOPY pubsub-to-bigquery.py ./\\nCOPY seventyfive ./seventyfive\\n\\nCMD python3 ./pubsub-to-bigquery.py\\n\")), mdx(\"p\", null, \"This uses the slim version of the python 3.7.2 image (as Marcus had recommended).  Python dependencies are then loaded from the requirements.txt file which uses the version info of the dependencies we manually installed earlier:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"File: requirements.txt\"))), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"google-cloud-pubsub==0.39.1\\ngoogle-cloud-bigquery==1.10.0\\noauth2client==4.1.3\\n\")), mdx(\"p\", null, \"It then runs the \\\"pubsub-to-bigquery.py\\\" program.\"), mdx(\"p\", null, \"I am able to build the image locally using:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"docker build -t baler-sensor-pubsub:1.0-SNAPSHOT .\\n\\ngcloud auth configure-docker\\ndocker tag my-image:1.0-SNAPSHOT gcr.io/my-image:1.0-SNAPSHOT\\ndocker push gcr.io/my-image:1.0-SNAPSHOT\\n\")), mdx(\"p\", null, \"I can then login to the GCE instance (which is using the container-optimized OS), download, and run this image with:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"docker-credential-gcr configure-docker\\ndocker pull gcr.io/my-image:1.0-SNAPSHOT\\ndocker run --rm gcr.io/my-image:1.0-SNAPSHOT\\n\")), mdx(\"p\", null, \"However, I'm running into the following errors:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/bigquery/v2/projects/<project>/datasets/<dataset>/tables/<table>: Request had insufficient authentication scopes.\\n\")), mdx(\"p\", null, \"I was able to run this successfully in my sandbox project.  It's the exact same docker image, but it fails in our corporate dev project.  It looks like our GCE instance does not have the required access to the BigQuery APIs.\"));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#google-tutorial-using-gke","title":"Google Tutorial using GKE","items":[{"url":"#final-code-files","title":"Final Code Files"},{"url":"#gcp-costs","title":"GCP Costs"}]},{"url":"#python-code-on-docker","title":"Python Code on Docker","items":[{"url":"#install-python-libraries","title":"Install Python Libraries"},{"url":"#bigquery-setup","title":"BigQuery Setup"},{"url":"#python-scripts","title":"Python Scripts"},{"url":"#writing-to-pubsub-from-on-prem","title":"Writing to PubSub from On-Prem"},{"url":"#deploying-as-a-docker-container","title":"Deploying as a Docker Container"}]}]},"parent":{"relativePath":"google-cloud-platform/pubsub-streaming-to-bigquery.md"},"frontmatter":{"metaTitle":null,"metaDescription":null}},"allMdx":{"edges":[{"node":{"fields":{"slug":"/class-notes","title":"Class Notes"}}},{"node":{"fields":{"slug":"/computer-technology","title":"Computer Technology"}}},{"node":{"fields":{"slug":"/google-cloud-platform","title":"Google Cloud Platform"}}},{"node":{"fields":{"slug":"/","title":"Dan's Notes"}}},{"node":{"fields":{"slug":"/lean-process-management","title":"Lean Process Management"}}},{"node":{"fields":{"slug":"/personal","title":"About Me"}}},{"node":{"fields":{"slug":"/class-notes/mit-supply-chain-management","title":"MIT Supply Chain Management"}}},{"node":{"fields":{"slug":"/class-notes/university-of-cincinnati","title":"University of Cincinnati"}}},{"node":{"fields":{"slug":"/computer-technology/builder-pattern-java","title":"The Builder Pattern in Java"}}},{"node":{"fields":{"slug":"/computer-technology/databases","title":"Databases"}}},{"node":{"fields":{"slug":"/computer-technology/docker-and-containers","title":"Docker and Containers"}}},{"node":{"fields":{"slug":"/computer-technology/docker","title":"Docker"}}},{"node":{"fields":{"slug":"/computer-technology/git","title":"Git"}}},{"node":{"fields":{"slug":"/computer-technology/gpl-beer-clause-license","title":"GPL License with Modified \"Beer Clause\""}}},{"node":{"fields":{"slug":"/computer-technology/html-css","title":"HTML and CSS"}}},{"node":{"fields":{"slug":"/computer-technology/proxy-firewall","title":"Proxy Firewalls"}}},{"node":{"fields":{"slug":"/computer-technology/r-and-docker","title":"R and Docker"}}},{"node":{"fields":{"slug":"/computer-technology/react","title":"React Notes"}}},{"node":{"fields":{"slug":"/computer-technology/regular-expressions","title":"Regular Expression Cheat Sheet"}}},{"node":{"fields":{"slug":"/computer-technology/tao-of-programming","title":"The Tao of Programming"}}},{"node":{"fields":{"slug":"/google-cloud-platform/cloud-sql","title":"Cloud SQL"}}},{"node":{"fields":{"slug":"/google-cloud-platform/cloud-storage-in-gcp","title":"Cloud Storage in GCP"}}},{"node":{"fields":{"slug":"/google-cloud-platform/coursera-courses-for-gcp","title":"Coursera Courses for GCP"}}},{"node":{"fields":{"slug":"/google-cloud-platform/gcp-regions","title":"GCP Regions"}}},{"node":{"fields":{"slug":"/google-cloud-platform/google-cloud-sdk","title":"Google Cloud SDK (gcloud and gsutil)"}}},{"node":{"fields":{"slug":"/google-cloud-platform/google-container-registry","title":"GCR (Google Container Registry)"}}},{"node":{"fields":{"slug":"/google-cloud-platform/pubsub-streaming-to-bigquery","title":"PubSub Streaming to BigQuery"}}},{"node":{"fields":{"slug":"/google-cloud-platform/virtual-private-cloud","title":"Virtual Private Cloud (VPC)"}}},{"node":{"fields":{"slug":"/lean-process-management/a3-problem-solving","title":"A3 Problem Solving"}}},{"node":{"fields":{"slug":"/lean-process-management/cause-and-effect-matrix","title":"Cause and Effect Matrix"}}},{"node":{"fields":{"slug":"/lean-process-management/fishbone-diagram","title":"Fishbone Diagram"}}},{"node":{"fields":{"slug":"/lean-process-management/five-whys","title":"5 Why's & Issue Trees"}}},{"node":{"fields":{"slug":"/lean-process-management/mece-diagram","title":"MECE Diagram"}}},{"node":{"fields":{"slug":"/lean-process-management/six-step-problem-solving","title":"Six Step Problem Solving"}}},{"node":{"fields":{"slug":"/lean-process-management/tbp-coaching-tool","title":"TBP Coaching Tool"}}},{"node":{"fields":{"slug":"/lean-process-management/weighted-shortest-job-first","title":"Weighted Shortest Job First"}}},{"node":{"fields":{"slug":"/lean-process-management/work-stories-and-eracs","title":"Work Stories and ERCS (Mr. Potato Head)"}}},{"node":{"fields":{"slug":"/personal/quotes","title":"Quotes"}}},{"node":{"fields":{"slug":"/personal/reading-list","title":"Reading List"}}},{"node":{"fields":{"slug":"/personal/work_rules","title":"Work Rules! (Notes)"}}},{"node":{"fields":{"slug":"/class-notes/mit-supply-chain-management/sc1-supply-chain-fundamentals","title":"CTL.SC1 Supply Chain Fundamentals"}}},{"node":{"fields":{"slug":"/class-notes/mit-supply-chain-management/sc4-supply-chain-tech-and-systems","title":"CTL.SC4 Supply Chain Technology & Systems"}}},{"node":{"fields":{"slug":"/class-notes/university-of-cincinnati/22QA725-time-series-analysis","title":"22 QA 725 - Time Series Analysis"}}},{"node":{"fields":{"slug":"/google-cloud-platform/bigquery","title":"BigQuery"}}},{"node":{"fields":{"slug":"/class-notes/mit-supply-chain-management/sc4-supply-chain-tech-and-systems/the-fresh-connection","title":"The Fresh Connection"}}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"id":"c2b6b9aa-66f8-587c-8e67-0d12c3444f15"}}}